You are an expert in Cypher and Neo4j. I need your help to interpret and model ESG data structured as a JSON object in the following format:

{ "all_data": data }

Here, data is a list of lists, where each inner list corresponds to a row of data, with the first row as headers.

Objective: Generate a flexible Cypher query that dynamically analyzes the input structure to determine core entities and relationships, create an appropriate Neo4j model, and import the data. This model should adapt to varying data structures, only assuming that fields related to ESG fall within the categories of Environmental, Social, or Governance metrics.

Phase 1: Data Analysis and Model Inference
1. Interpret the Data Structure:

* Analyze the header row to identify potential entities and relationships within the data, using only the headers and a sampling of values in subsequent rows.
* Infer the following:
    * Entity Types: Detect primary entities (such as a company or organization) that should serve as unique nodes.
    * Date or Time: Identify any fields that appear to represent dates or years, likely used to distinguish different records or reports.
    * Categories: Determine fields that may serve as categories or classifications (e.g., industry, sector) for grouping related entities.
    * ESG-Related Fields: Identify fields with ESG relevance, categorizing them under Environment, Social, or Governance.
2.Define Model Structure:

* Create a description of the model based on the inferred entities and relationships:
    * Primary Entity Nodes: For each unique primary entity, create a node (e.g., a Company or other identified entity).
    * Report Nodes: For each row in the data, create a Report node, linking it to the primary entity node and any date or year nodes.
    * ESG Nodes: Group ESG-related fields into Environment, Social, and Governance nodes. Link these nodes to each Report node based on the inferred ESG fields.
    * Categorical Nodes: For any identified categories (e.g., industry or sector), create nodes and link relevant entities to these categories.
    * Date/Year Nodes: Create nodes to represent years, allowing linkage of reports to their corresponding years.
3. Output Model Description: Based on the analysis, output a high-level description of the entities, relationships, and categories you plan to create, highlighting any inferred relationships without referencing specific field names.

Here is the input data to begin:
{
"all_data": {{ csv_input }}
}

Phase 2: Cypher Query Generation
Using the model structure from Phase 1, create a Cypher query to import the data, adhering to the following guidelines:

1. Data Processing Instructions:

* Start with data.all_data[0] as headers and process only the data rows.
* Construct mappings between headers and values manually without using apoc.map.fromLists.
* Set default values for missing or empty fields:
    * Use COALESCE(value, 'Unknown') for text fields.
    * Use COALESCE(value, 0) for numeric fields.
2. Node and Relationship Creation:

* Create nodes for each entity inferred from the data, linking to any category or industry nodes.
* Create Report nodes for each data row and link them to the relevant primary entity, date, and ESG nodes.
* For each ESG-related field, create separate Environment, Social, and Governance nodes linked to the Report.
* Create Year nodes based on date fields and establish relationships to organize reports by year.
3. Output JSON Format:

* The JSON output should include:
    * "cypher_query": The generated Cypher query, which should be flexible and reference fields generically (e.g., entry.FieldName).
    * "explanation": A summary of the model's entities and relationships.
Example Output:
{ "cypher_query": "YOUR CYPHER QUERY HERE", "explanation": "YOUR EXPLANATION HERE" }

Important Notes:
* Flexibility: The query should be adaptable to various data structures, referencing fields generically and avoiding hard-coded field names (except for ESG categories).
* Default Values: Apply default values for missing or empty fields as described.
* Valid JSON: Ensure the JSON output is valid with no extraneous characters or line breaks.